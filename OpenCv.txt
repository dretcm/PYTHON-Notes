----------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------- GUIDE OpenCv -------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------

1 -------------->>> Images (display, resize, rotate, save, flip, shape):

# read images:

import cv2
path = 'c:/users/usuario/pictures/dog_aux.jpg'

# -1 : cv2.IMREAD_COLOR : loads a color image(its by default).
# 0 : cv2.IMREAD_GRAYSCALE : loads a image in grayscale mode.
# 1 : cv2.IMREAD_UNCHANGED : loads image as such including alpha channel.

img = cv2.imread(path, -1)

cv2.imshow('Image', img) # (title, tensor)
cv2.waitKey(0)   # 0 : infinity time, until to press whatever key; 1000 : is equivalent to 10 seconds; 5000 : to 5 seconds, etc. # and returns key's number in ascii.
cv2.destroyAllWindows()

# resize images:

import cv2
path = 'c:/users/usuario/pictures/dog_aux.jpg'

img = cv2.imread(path, 1)
img = cv2.resize(img, (500, 400))  # (tensor, (width, height))

cv2.imshow('Image', img)
cv2.waitKey(0)
cv2.destroyAllWindows()

# resize according the porcentage:

import cv2
path = 'c:/users/usuario/pictures/dog_aux.jpg'

img = cv2.imread(path, 1)
img = cv2.resize(img, (0,0), fx=0.5, fy =0.4)  # (tensor, (width, height), porcentage of width according the original cap, porcentage of height according the original cap)

cv2.imshow('Image', img)
cv2.waitKey(0)
cv2.destroyAllWindows()

# rotate images and save images:

import cv2
path = 'c:/users/usuario/pictures/dog_aux.jpg'

img = cv2.imread(path, 1)
img = cv2.resize(img, (400, 400))

# cv2.ROTATE_90_CLOCKWISE
# cv2.ROTATE_90_COUNTERCLOCKWISE
# cv2.ROTATE_180

img = cv2.rotate(img, cv2.cv2.ROTATE_90_CLOCKWISE)   # clockwise(en sentido horario)

cv2.imwrite('new_img.jpg', img)   # save image (filename, tensor)

cv2.waitKey(0)
cv2.destroyAllWindows()

# flip:

## flipcode = 0: flip vertically
## flipcode = 1: flip horizontally
## flipcode = -1 : flip vertically and horizontally

import cv2
path = 'c:/users/usuario/pictures/dog_aux.jpg'

img = cv2.imread(path)
img = cv2.flip(img, -1)

cv2.imshow('Image', img)

cv2.waitKey(0)
cv2.destroyAllWindows()

# shape:

import cv2
path = 'c:/users/usuario/pictures/dog_aux.jpg'
img = cv2.imread(path)

print(img.shape) # output: (1200, 900, 3) # (height, width, channels(RGB)) # (tensors, rows, columns)

2 --------------->>> Image Fundamentals and Manipulation :

# array:

import cv2
img = cv2.imread('dog.png')
print(img) # array numpy
print(type(img)) # <class 'numpy.ndarray'>


# 3 channels(standar RGB, open-cv use BGR) to 1 (GrayScale):

img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)


# changue to random color in a part of image:

import cv2
import numpy as np

img = cv2.imread('dog.png')
img = cv2.resize(img, (400,400))

for i in range(100): # vectical shift
        for j in range(200): # horizontal shift
                img[i][j] = [np.random.randint(0,256), np.random.randint(0,256), np.random.randint(0,256)] # 3 channels [blue, green, red]

cv2.imshow('Image', img)
cv2.waitKey(0)
cv2.destroyAllWindows()


# BGR colors imshow of image:

import cv2
img = cv2.imread('dog.png')
img = cv2.resize(img, (400,500))

for i in range(3):
        aux = img.copy()
        for j in range(3):
                if i != j:
                        aux[:,:,j] = 0    # only one channel will have colors, the others will be matched to '0'.
        cv2.imshow('Image', aux)
        cv2.waitKey(0)
        
cv2.destroyAllWindows()


# coping and passing parts of image:

import cv2
img = cv2.imread('dog.png')
img = cv2.resize(img, (400,500))

aux = img[100:250,100:200]  # select part.
img[:150,:100] = aux   # replace part by 'aux'.

cv2.imshow('Image', img)
cv2.waitKey(0)  
cv2.destroyAllWindows()

3 --------------->>> cameras and video captures:

# display video and capture in device:

import cv2
capture = cv2.VideoCapture('BEASTARS.mp4')   # cv2.VideoCapture(0) for camera.

if not capture.isOpened(): # (True/False) if open return True.
    	print("Cannot open camera")
    	exit()

cv2.namedWindow("frame", cv2.WINDOW_NORMAL)  # define the name of display.
cv2.resizeWindow("frame", 480, 270)          # (display name to configurate, width, height).

while (True):
        ret, frame = capture.read()   # returns (bool (True/False) If the frame is read correctly, and the frame).

        if not ret:
                break

        cv2.imshow('frame',frame)
        
        if cv2.waitKey(1) == ord('q'):
                break
        
capture.release()  # close the capture or finish it.
cv2.destroyAllWindows()


# other form of resize frame:

import cv2
capture = cv2.VideoCapture('BEASTARS.mp4')

while (True):
        ret, frame = capture.read()

        if not ret:
                break

        frame = cv2.resize(frame, (480, 270)) # other form of resize frame (width, height)
        cv2.imshow('frame',frame)
        
        if cv2.waitKey(1) == ord('q'):
                break

capture.release()
cv2.destroyAllWindows()


# Properties camera, only with camera! :

0. CV_CAP_PROP_POS_MSEC Current position of the video file in milliseconds.
1. CV_CAP_PROP_POS_FRAMES 0-based index of the frame to be decoded/captured next.
2. CV_CAP_PROP_POS_AVI_RATIO Relative position of the video file
3. CV_CAP_PROP_FRAME_WIDTH Width of the frames in the video stream.
4. CV_CAP_PROP_FRAME_HEIGHT Height of the frames in the video stream.
5. CV_CAP_PROP_FPS Frame rate.
6. CV_CAP_PROP_FOURCC 4-character code of codec.
7. CV_CAP_PROP_FRAME_COUNT Number of frames in the video file.
8. CV_CAP_PROP_FORMAT Format of the Mat objects returned by retrieve() .
9. CV_CAP_PROP_MODE Backend-specific value indicating the current capture mode.
10. CV_CAP_PROP_BRIGHTNESS Brightness of the image (only for cameras).
11. CV_CAP_PROP_CONTRAST Contrast of the image (only for cameras).
12. CV_CAP_PROP_SATURATION Saturation of the image (only for cameras).
13. CV_CAP_PROP_HUE Hue of the image (only for cameras).
14. CV_CAP_PROP_GAIN Gain of the image (only for cameras).
15. CV_CAP_PROP_EXPOSURE Exposure (only for cameras).
16. CV_CAP_PROP_CONVERT_RGB Boolean flags indicating whether images should be converted to RGB.
17. CV_CAP_PROP_WHITE_BALANCE Currently unsupported
18. CV_CAP_PROP_RECTIFICATION Rectification flag for stereo cameras (note: only supported by DC1394 v 2.x backend currently)

example:

import cv2

cap = cv2.VideoCapture(0)
cap.set(3,600) # 3 for width, 4 for height
cap.set(4,500)

cap.get(3,600) # 3 for width, 4 for height
cap.get(4,500)

while True:
    ret, frame = cap.read()

    cv2.imshow('frame', frame)

    if cv2.waitKey(20) & 0xFF == ord('q'): # if press the key 'q' finish capture.
        break

cap.release()
cv2.destroyAllWindows()


# video multiple times with a camera:

import cv2
import numpy as np

cap = cv2.VideoCapture('BEASTARS.mp4')

width = int(cap.get(3))
height = int(cap.get(4))

image = np.zeros((height, width, 3), np.uint8)

width //=2  # divide the image in 4 parts.
height //=2
        
while (cap.isOpened()):
        ret, frame = cap.read()

        
        frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)

        image[:height,:width] = frame
        image[:height, width:] = frame
        image[height:,:width] = frame
        image[height:, width:] = frame
        
        
        cv2.imshow('frame',image)
        
        if cv2.waitKey(1) == ord('q'):
                break

cap.release()
cv2.destroyAllWindows()

# video multiple times with a video, rotate and flip:

import cv2
import numpy as np

cap = cv2.VideoCapture('BEASTARS.mp4')
cv2.namedWindow("frame", cv2.WINDOW_NORMAL)
cv2.resizeWindow("frame", 880, 570)

width = int(cap.get(3))
height = int(cap.get(4))

image = np.zeros((height, width, 3), np.uint8)

width //=2
height //=2
        
while (cap.isOpened()):
        ret, frame = cap.read()

        if not ret:
                break
        
        frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)
        
        image[:height,:width] = cv2.rotate(frame, cv2.cv2.ROTATE_180)
        image[:height, width:] = cv2.flip(frame, 0)
        image[height:,:width] = cv2.flip(frame,1)
        image[height:, width:] = frame
        
        
        cv2.imshow('frame',image)
        
        if cv2.waitKey(1) == ord('q'):
                break

cap.release()
cv2.destroyAllWindows()

4 --------------->>> Drawing (Lines, Images, Circles & Text) : draw tringle - draw contours :

# example 1:

import cv2
cap = cv2.VideoCapture(0)
width = int(cap.get(3))
height = int(cap.get(4))

while (True):
        ret, frame = cap.read()

        img = cv2.line(frame, (0,0), (width,height), (255,0,0), 15) # (tensor or image, (x start,y start), (x end, y end), colors(BGR), line_width)
        img = cv2.line(img, (0,height), (width, 0), (0,255,0), 5)
        img = cv2.rectangle(img, (150,10), (200,450), (0,128,255), 10) # (tensor or image, (x start,y start), (x end, y end), colors(BGR), line_width)
        img = cv2.circle(img, (300,300), 100, (0,0,255), -1) # (tensor or image, (x center,y center), radius, colors(BGR),  -1:if want fill circle - 0...12..100:only border)

        font = cv2.FONT_HERSHEY_SIMPLEX        # type of font.
        img = cv2.putText(img, 'Hello :b', (10,450), font, 3, (255,255,255), 10, cv2.LINE_AA) # (tensor, text, (x,y), previous font, line scale, BGR colors, bold line, type line)

        cv2.imshow('frame',img)
        
        if cv2.waitKey(1) == ord('q'):
                break
        
cap.release()
cv2.destroyAllWindows()


# draw triangle:

import cv2
import numpy as np

cap = cv2.VideoCapture(0)

triangle_position = np.array([(300,10),(160,200),(440,200)])   # midle top, left bot, right bot.

while (True):
        ret, frame = cap.read()

        cv2.drawContours(frame, [triangle_position], 0, (0,255,0), 0)  # (tensor, array with the position, 0 pls, BGR colors, -1:if want fill triangle - 0...12..100:only border)

        cv2.imshow('frame',frame)
        
        if cv2.waitKey(1) == ord('q'):
                break
        
cap.release()
cv2.destroyAllWindows()


5 --------------->>> Colors and Color Detection :

# convert to HSV (which stands for Hue Saturation Value, Hue is measured in degrees from 0 to 360) : 
import cv2

cap = cv2.VideoCapture(0)

while (True):
        ret, frame = cap.read()

        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

        cv2.imshow('frame',frame)
        
        if cv2.waitKey(1) == ord('q'):
                break
        
cap.release()
cv2.destroyAllWindows()


# show filter camera colors : mask :
# link of replace green background: https://programmerclick.com/article/54071808408/

import cv2
import numpy as np

cap = cv2.VideoCapture(0)

while (True):
        ret, frame = cap.read()

        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)  # hue saturation value
        
        lower_blue = np.array([90,50,50])  # range colors to filter
        upper_blue = np.array([128,255,255])
        
        mask = cv2.inRange(hsv, lower_blue, upper_blue)  # in this case is for blue colors.

        result = cv2.bitwise_and(frame,frame,mask=mask)
        
        cv2.imshow('frame',result)
        cv2.imshow('mask',mask)
        
        if cv2.waitKey(1) == ord('q'):
                break
        
cap.release()
cv2.destroyAllWindows()



# filter laplacian :

import cv2
import numpy as np


cap = cv2.VideoCapture(path)
cv2.namedWindow("frame", cv2.WINDOW_NORMAL)
cv2.resizeWindow("frame", 480, 270) 

while True:
        _, frame = cap.read()

        laplacian = cv2.Laplacian(frame,cv2.CV_64F)
        laplacian = np.uint8(laplacian)

        cv2.imshow('frame',laplacian)

        if cv2.waitKey(1) == ord('q'):
                break

cap.release()
cv2.destroyAllWindows()


# filter canny :

import cv2
import numpy as np


cap = cv2.VideoCapture(path)
cv2.namedWindow("frame", cv2.WINDOW_NORMAL)
cv2.resizeWindow("frame", 480, 270) 

while True:
        _, frame = cap.read()

        edges = cv2.Canny(frame,100,200) # (frame, minvalue, max value)

        cv2.imshow('frame',edges)

        if cv2.waitKey(1) == ord('q'):
                break

cap.release()
cv2.destroyAllWindows()


6 --------------->>> corner detection : Track : track:

# example 1 : if you want to know more, search in google.

import numpy as np
import cv2
import random

img = cv2.imread('chess.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 10) # (image source, number of corners, minimum quality, minimunm euclidean distance)
corners = np.int0(corners)

for corner in corners:
        x, y = corner.ravel()
        cv2.circle(img, (x,y), 5, (255,0,0), -1)
cv2.imshow('img',img)

cv2.waitKey(0)
cv2.destroyAllWindows()


7 --------------->>> Templates Matching : template matching :

import numpy as np
import cv2

img = cv2.resize(cv2.imread('soccer_practice.jpg', 0), (0, 0), fx=0.5, fy=0.5)
template = cv2.resize(cv2.imread('ball.png', 0), (0, 0), fx=0.5, fy=0.5)
h, w = template.shape

# Mathematic templates
methods = [cv2.TM_CCOEFF, cv2.TM_CCOEFF_NORMED, cv2.TM_CCORR,
            cv2.TM_CCORR_NORMED, cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]

for method in methods:
        aux = img.copy()

        result = cv2.matchTemplate(aux, template, method)
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
        
        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
                location = min_loc
        else:
                location = max_loc

        bottom_right = (location[0] + w, location[1] + h)
        cv2.rectangle(aux, location, bottom_right, 255, 5)
        
        cv2.imshow('Match', aux)

        cv2.waitKey(0)
        cv2.destroyAllWindows()


8 --------------->>> Face and Eye Detection : 

import numpy as np
import cv2

cap = cv2.VideoCapture(0)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

while True:
        ret, frame = cap.read()

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        
        for (x, y, w, h) in faces:
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 5)
                roi_gray = gray[y:y+w, x:x+w]
                roi_color = frame[y:y+h, x:x+w]
                eyes = eye_cascade.detectMultiScale(roi_gray, 1.3, 5)
                for (ex, ey, ew, eh) in eyes:
                        cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 5)

        cv2.imshow('frame', frame)

        if cv2.waitKey(1) == ord('q'):
                break

cap.release()
cv2.destroyAllWindows()



# option 2:

import cv2
import dlib

# Load the detector
detector = dlib.get_frontal_face_detector()

# read the image
img = cv2.imread("face.jpg")

# Convert image into grayscale
gray = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2GRAY)

# Use detector to find landmarks
faces = detector(gray)

for face in faces:
    x1 = face.left() # left point
    y1 = face.top() # top point
    x2 = face.right() # right point
    y2 = face.bottom() # bottom point
    # Draw a rectangle
    cv2.rectangle(img=img, pt1=(x1, y1), pt2=(x2, y2), color=(0, 255, 0), thickness=4)

# show the image
cv2.imshow(winname="Face", mat=img)

# Wait for a key press to exit
cv2.waitKey(delay=0)

# Close all windows
cv2.destroyAllWindows()



# mouth :
import cv2
import sys

mouthCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

video_capture = cv2.VideoCapture(0)

while True:
    ret, frame = video_capture.read()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    mouth = mouthCascade.detectMultiScale(gray, 1.3, 5)
    faces = faceCascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30)
            )

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)
        roi_gray_mouth = gray[y+(int(h/2)):y+h, x:x+w]
        roi_color_mouth = frame[y+(int(h/2)):y+h, x:x+w]

        mouth = mouthCascade.detectMultiScale(roi_gray_mouth)

        for (ex,ey,ew,eh) in mouth:
            cv2.rectangle(roi_color_mouth, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)
            break

    cv2.imshow('Video', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

video_capture.release()
cv2.destroyAllWindows()


9 --------------->>> get time of video : get number of fps : get number of frames :

import cv2

cap = cv2.VideoCapture("dancing_dog.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)    # OpenCV2 version 2 used "CV_CAP_PROP_FPS" get the "fps".
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))   # get the number of frames.
duration = int(frame_count/fps)  # FPS :  Frames Per Seconds

print('fps = ', fps)
print('number of frames = ', frame_count)
print('duration (Seconds) = ', duration)
hours = int(duration/3600)
duration = duration % 3600
minutes = int(duration/60)
seconds = duration%60
print('{:02}:{:02}:{:02}'.format(hours,minutes,seconds))

cap.release()

10 --------------->>> 
11 --------------->>> 
12 --------------->>> 

20 -------------->>> save or recording videos :

import cv2 
import pyautogui
import numpy as np
from skvideo import io

if __name__ == '__main__':
    codec = cv2.VideoWriter_fourcc(*"XVID")
    out = cv2.VideoWriter("D:\Grabacion.avi", codec , 20.0, (640, 440))  # (outputfile, codec, fps, size_video)

    cv2.namedWindow("Grabando", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("Grabando", 480, 270) #Los ultimos dos argunmentos son las dimensiones de la ventana de grabación

    while True:
        img = pyautogui.screenshot() # tomamos un pantallazo
        frame = np.asarray(img) # convertimos la imagen a un arreglo de numeros
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convertimos la imagen BGR a RGB
        cv2.imshow('Grabando', frame) # mostramos el cuadro que acabamos de grabar
        out.write(frame)
        if cv2.waitKey(1) == ord('q'): # si el usuario presiona 'q' paramos de grabar.
            break

    out.release() # cerrar el archivo de video
    cv2.destroyAllWindows() # cerrar la ventana


# option 2(with skvideo - pip install scikit-video):
# more about skvideo: http://www.scikit-video.org/stable/io.html

import cv2 # OpenCV
import pyautogui
import numpy as np
from skvideo import io

if __name__ == '__main__':

    writer = io.FFmpegWriter("D:\Grabacion.avi", outputdict={
      '-vcodec': 'libx264',  #use the h.264 codec
      '-crf': '0',           #set the constant rate factor to 0, which is lossless
      '-preset':'veryslow'   #the slower the better compression, in princple, try 
                         #other options see https://trac.ffmpeg.org/wiki/Encode/H.264
    }) 

    cv2.namedWindow("Grabando", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("Grabando", 480, 270) #Los ultimos dos argunmentos son las dimensiones de la ventana de grabación

    while True:
        img = pyautogui.screenshot() # tomamos un pantallazo
        frame = np.asarray(img) # convertimos la imagen a un arreglo de numeros

        cv2.imshow('Grabando', frame) # mostramos el cuadro que acabamos de grabar

        writer.writeFrame(frame)

        if cv2.waitKey(1) == ord('q'): # si el usuario presiona 'q' paramos de grabar.
            break

    writer.close()
    cv2.destroyAllWindows() # cerrar la ventana


# create a video with images in your disk:

path = r"D:\images"

def generate_video(path, lenght):

        writer = io.FFmpegWriter("D:\Grabacion.avi", outputdict={
              '-vcodec': 'libx264',  #use the h.264 codec
              '-crf': '0',           #set the constant rate factor to 0, which is lossless
              '-preset':'veryslow'   #the slower the better compression, in princple, try 
                                 #other options see https://trac.ffmpeg.org/wiki/Encode/H.264
        })
        
        cv2.namedWindow("Grabando", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("Grabando", 480, 270)
        
        for i in range(lenght):
            img = cv2.imread(path+'\\'+f'img{i}.jpg')
            frame = np.asarray(img)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            writer.writeFrame(frame[:,:,::-1])  # cv2 use BGR, we want RGB for this we use "::-1" for write the frame in reverse "RGB".
            cv2.imshow('Grabando', frame)
            
        write.close()
        cv2.destroyAllWindows()

generate_video(path,361)

21 ---------------->>> Screen Recorder :
import cv2 
import pyautogui
import numpy as np
import time

if __name__ == '__main__':
    codec = cv2.VideoWriter_fourcc(*"XVID")
    out = cv2.VideoWriter("ScreenRecorder.avi", codec , 20.0, (640, 440))

    prev = 0
    fps = 120
    while True:
	time_elapsed = time.time() - prev

        img = pyautogui.screenshot()
	
	if time_elapsed > 1.0/fps:
		prev = time.time()
        	frame = np.asarray(img)
        	frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        	out.write(frame)
	cv2.waitKey(100)

    out.release()
    cv2.destroyAllWindows()

22 ---------------->>> GREEN BACKOGROUND VIDEO REPLCAE : MASK : 

import cv2
import numpy as np
import subprocess
import os

def video(path):
        cap = cv2.VideoCapture('lonrot.mp4')
        width, height = 854, 480

        lower_green = np.array([0,80,0])   # threshold of filter coloer green
        upper_green = np.array([80,255,80])

        img = cv2.imread(path)
        img = cv2.resize(img, (width,height))  # resize the image to the size of video.

        
        codec = cv2.VideoWriter_fourcc(*"MP4V")
        out = cv2.VideoWriter("video.mp4", codec , 30, (width,height))

        while (True):
                try:
                        ret, frame = cap.read()

                        if not ret:
                                break

                        mask = cv2.inRange(frame.copy(), lower_green, upper_green)
                                
                        frame[mask != 0] = [0, 0, 0]
                                
                        image = img.copy()
                        image[mask==0] = [0,0,0]

                        final = image + frame
                                
                        cv2.imshow('video',final)
                                
                        out.write(final)
                                
                        if cv2.waitKey(1) == ord('q'):
                                break
                except Exception as e:
                        print(e)
                        break
                
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        

def generate_video(path, outputfile = "output.mp4"):
        video(path)
        videofile = "video.mp4"
        audiofile = "audio.mp3"
        aux = "temporal.mp4"
        codec = "copy"
        
        subprocess.run(f"ffmpeg -i {videofile} -i {audiofile} -c {codec} {aux}")
        os.system(f'DEL /F /A {videofile}')
        subprocess.run(f"ffmpeg -i {aux} -c:v libx264 -b:v 1.5M -c:a libvo_aacenc -b:a 128K {outputfile}")
        os.system(f'DEL /F /A {aux}')
        

if __name__ == '__main__':
        path = 'diego.jpg'
        generate_video(path, outputfile='resulted_video.mp4')

23 ---------------->>> facial landmark detection : 

https://livecodestream.dev/post/detecting-face-features-with-python/

import cv2
import dlib

# Load the detector
detector = dlib.get_frontal_face_detector()

# Load the predictor
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

# read the image
cap = cv2.VideoCapture(0)

while True:
    _, frame = cap.read()
    # Convert image into grayscale
    gray = cv2.cvtColor(src=frame, code=cv2.COLOR_BGR2GRAY)

    # Use detector to find landmarks
    faces = detector(gray)

    for face in faces:
        x1 = face.left()  # left point
        y1 = face.top()  # top point
        x2 = face.right()  # right point
        y2 = face.bottom()  # bottom point

        # Create landmark object
        landmarks = predictor(image=gray, box=face)

        # Loop through all the points
        for n in range(0, 68):
            x = landmarks.part(n).x
            y = landmarks.part(n).y

            # Draw a circle
            cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 255, 0), thickness=-1)

    # show the image
    cv2.imshow(winname="Face", mat=frame)

    # Exit when escape is pressed
    if cv2.waitKey(delay=1) == 27:  # if push the key escape.
        break

# When everything done, release the video capture and video write objects
cap.release()

# Close all windows
cv2.destroyAllWindows()


23 ---------------->>> hand recognition and his points :

import cv2
import mediapipe as mp

mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands
cap = cv2.VideoCapture(0)

with mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5) as hands:
        while True:
                ret, frame = cap.read()
                if ret == False:
                    break
                height, width, _ = frame.shape
                frame = cv2.flip(frame, 1)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = hands.process(frame_rgb)
                if results.multi_hand_landmarks is not None:
                        for hand_landmarks in results.multi_hand_landmarks:
                		mp_drawing.draw_landmarks(
                    			frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                    			mp_drawing.DrawingSpec(color=(0,255,255), thickness=3, circle_radius=5),
                    			mp_drawing.DrawingSpec(color=(255,0,255), thickness=4, circle_radius=5))
                                
                cv2.imshow('Frame',frame)
                if cv2.waitKey(1) & 0xFF == 27:
                    break
                
cap.release()
cv2.destroyAllWindows()


## prediction: 
print('Handedness:', results.multi_handedness) # return a list of index, accuracy_predict, predict


## fingers :

import cv2
import mediapipe as mp

mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands
cap = cv2.VideoCapture(0)

with mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5) as hands:
        while True:
                ret, frame = cap.read()
                if ret == False:
                    break
                height, width, _ = frame.shape
                frame = cv2.flip(frame, 1)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = hands.process(frame_rgb)
                if results.multi_hand_landmarks is not None:
			# THUMB_TIP, INDEX_FINGER_TIP, MIDDLE_FINGER_TIP, RING_FINGER_TIP y  PINKY_TIP = (pulgar, indice, dedo medio, anular, meñique).
		        x1 = int(hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x * width)
		        y1 = int(hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y * height)
			x2 = int(hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * width)
			y2 = int(hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * height)
			x3 = int(hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].x * width)
			y3 = int(hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y * height)
			x4 = int(hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].x * width)
			y4 = int(hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].y * height)
			x5 = int(hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].x * width)
			y5 = int(hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].y * height)
			cv2.circle(image, (x1, y1), 3,(255,0,0),3)
			cv2.circle(image, (x2, y2), 3,(255,0,0),3)
			cv2.circle(image, (x3, y3), 3,(255,0,0),3)
			cv2.circle(image, (x4, y4), 3,(255,0,0),3)
			cv2.circle(image, (x5, y5), 3,(255,0,0),3)
                          
                cv2.imshow('Frame',frame)
                if cv2.waitKey(1) & 0xFF == 27:
                    break
                
cap.release()
cv2.destroyAllWindows()


23 ---------------->>>
23 ---------------->>>
23 ---------------->>>
23 ---------------->>>